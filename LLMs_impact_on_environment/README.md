# Impact of Large Language Models on environment

- Negative impacts of large language models :
    - Carbon footprint
    - Energy consumption

<br>

### What is Carbon Footprint

> Carbon footprint refers to the **total amount of greenhouse gases** emitted directly or indirectly by an activity, product, or organization, typically measured in terms of **carbon dioxide equivalents (CO‚ÇÇe)**.

- üåç **Greenhouse gases** such as water vapor, carbon dioxide (CO‚ÇÇ), chlorofluorocarbons (CFCs), nitrous oxide (N‚ÇÇO), and methane (CH‚ÇÑ) trap heat from the sun‚Äôs rays, warming the **troposphere** (the lowest layer of Earth‚Äôs atmosphere). This phenomenon is known as the **greenhouse effect**.
- üî• **Greenhouse Warming Potential (GWP)** refers to the impact a greenhouse gas has on global warming, based on:
  - Its **abundance**
  - Its **lifetime** in the atmosphere
- üïí **Carbon dioxide** has an atmospheric lifetime of **300‚Äì1000 years**, making it one of the **most harmful greenhouse gases**.
- üå°Ô∏è The increase in greenhouse gas emissions leads to **global warming**.
- üßÆ A **carbon footprint** accounts for all the greenhouse gas emissions caused by a specific **activity**, **product**, or **company**.
> ‚ö†Ô∏è **Anything that increases the carbon footprint poses a serious threat to environmental sustainability.**

<br>

### Carbon Footprint of Large Language Models

> Large Language Models (LLMs) have a significant carbon footprint due to their high energy consumption throughout their lifecycle.

- ‚ö° The **computing hardware** required to train and run LLMs consumes a **large amount of energy**.
- üîÑ Since **energy production itself generates emissions**, the energy consumed by LLMs indirectly contributes to their **carbon footprint**.
- üìä **Training a 175 billion parameter language model** can consume as much energy as required to power an **average American home for 40 years**.
- ‚öôÔ∏è LLMs consume energy during both:
  - **Training phase** ‚Äî when the model learns from data.
  - **Inference phase** ‚Äî when the model generates responses.
- üè¢ **Data centers** housing these models also contribute to emissions due to:
  - High **air conditioning needs**  
  - Continuous **power supply requirements**
- üèóÔ∏è The **manufacturing and maintenance** of infrastructure (servers, cooling systems, etc.) have their own carbon footprints and contribute to the **depletion of natural resources**.

> ‚úÖ **Reducing the carbon footprint** of LLMs requires:
  - Designing **efficient training algorithms**
  - Optimizing **data center cooling and energy management**  
  - Implementing solutions like the **BCOOLER algorithm** for energy-efficient training

<br>

### Different Types of Emissions Due to Large Language Models

> Training and deploying large language models contribute to various forms of carbon emissions, both direct and indirect.

#### 1. ‚öôÔ∏è Embodied Emissions  
Emissions associated with the **materials, production, and deployment** of computing infrastructure used in machine learning.
- **Dynamic Consumption**
  - Refers to the **electricity required to power the model during training**.
  - Measures the **energy consumed by servers** actively running training tasks.
- **Idle Power Consumption**
  - Energy consumed by **servers that are powered on but not actively used**.
  - Also includes energy used by supporting infrastructure just to **keep systems ready**.
- **Infrastructure Consumption**
  - Energy used by **data center infrastructure**, including:
    - Networking equipment  
    - Cooling systems  
    - Maintenance operations  
#### 2. üîÑ Operational Emissions  
Emissions resulting from **real-time use**, such as when models are deployed to handle user queries, including power required to run inference and maintain system availability.

<br>

### An Interesting Study

> üîç **Conclusion from Facebook AI Research:**  
> While the overall carbon footprint of machine learning models is currently smaller than many other sources, it is growing rapidly and will soon become a significant concern.  
> A rough power consumption breakdown of AI infrastructure shows **10% for experimentation, 20% for training, and 70% for inference**.  
> This reveals that **training emissions are just the tip of the iceberg**, and **inference ‚Äî the most frequently occurring phase ‚Äî is the largest contributor** to the long-term environmental impact of large language models.


<br>

## üíß Water Footprint

> In the context of AI, **water footprint** refers to the amount of water used, primarily for **cooling data centers**.

- Data centers housing large AI models require extensive **cooling**, which consumes significant amounts of water.
- Additionally, **electricity generation**, which powers these data centers, is the **second largest consumer of water**.
- Treating water and **wastewater also consumes electricity**, creating a feedback loop of resource usage.

---

## üõ¢Ô∏è Pollution

> The environmental impact of AI infrastructure extends to **pollution of soil and water**, as well as hazardous **e-waste generation**.

- Data centers may contribute to **soil and water pollution** through the **byproducts of cleaning and cooling systems**.
- **Manufacturing computing hardware** results in large amounts of **electronic waste (e-waste)**, posing **disposal and recycling challenges**.
- **Chip manufacturing** is particularly harmful, producing substantial amounts of **toxic waste**.

---

## üåç Resource Depletion

> The infrastructure used to train and deploy AI models relies heavily on **natural resources**, many of which are **non-renewable**.

- The **manufacturing and operation** of training, deployment, and inference hardware consume resources like:
  - **Coal**
  - **Natural gas**
  - **Fossil fuels**
  - **Nuclear energy**
- ‚ö†Ô∏è Roughly **85% of the world's energy** is generated from **non-renewable sources**, making AI development a contributor to **resource depletion**.
