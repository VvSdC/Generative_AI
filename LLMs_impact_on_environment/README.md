# Impact of Large Language Models on environment

- Negative impacts of large language models :
    - Carbon footprint
    - Energy consumption


### What is Carbon Footprint

> Carbon footprint refers to the **total amount of greenhouse gases** emitted directly or indirectly by an activity, product, or organization, typically measured in terms of **carbon dioxide equivalents (CO₂e)**.

- 🌍 **Greenhouse gases** such as water vapor, carbon dioxide (CO₂), chlorofluorocarbons (CFCs), nitrous oxide (N₂O), and methane (CH₄) trap heat from the sun’s rays, warming the **troposphere** (the lowest layer of Earth’s atmosphere). This phenomenon is known as the **greenhouse effect**.
- 🔥 **Greenhouse Warming Potential (GWP)** refers to the impact a greenhouse gas has on global warming, based on:
  - Its **abundance**
  - Its **lifetime** in the atmosphere
- 🕒 **Carbon dioxide** has an atmospheric lifetime of **300–1000 years**, making it one of the **most harmful greenhouse gases**.
- 🌡️ The increase in greenhouse gas emissions leads to **global warming**.
- 🧮 A **carbon footprint** accounts for all the greenhouse gas emissions caused by a specific **activity**, **product**, or **company**.
> ⚠️ **Anything that increases the carbon footprint poses a serious threat to environmental sustainability.**


### Carbon footprint of Large Language Models

- The computing machinery that powers the large language models consumes energy.
- Energy production is a process that has its own carbon footprint.
- Training a 175 billion parameter language model consumes energy that is enough to power an average american home for 40 years.
- Large Language Models consume energy during training and inference phases.
- The data centers that house the large language models may contribute additionally to greenhouse gas emissions due to air conditioning needs etc.

### Different Types of Emissions Due to Large Language Models

> Training and deploying large language models contribute to various forms of carbon emissions, both direct and indirect.

#### 1. ⚙️ Embodied Emissions  
Emissions associated with the **materials, production, and deployment** of computing infrastructure used in machine learning.
- **Dynamic Consumption**
  - Refers to the **electricity required to power the model during training**.
  - Measures the **energy consumed by servers** actively running training tasks.
- **Idle Power Consumption**
  - Energy consumed by **servers that are powered on but not actively used**.
  - Also includes energy used by supporting infrastructure just to **keep systems ready**.
- **Infrastructure Consumption**
  - Energy used by **data center infrastructure**, including:
    - Networking equipment  
    - Cooling systems  
    - Maintenance operations  
#### 2. 🔄 Operational Emissions  
Emissions resulting from **real-time use**, such as when models are deployed to handle user queries, including power required to run inference and maintain system availability.
