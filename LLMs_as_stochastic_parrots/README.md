# Large Language Models as Stochastic Parrots

### Introduction

- The race to build bigger, faster, and more capable AI models is pushing organizations to stretch the limits of current technology.
- Significant resources and investments are being directed toward achieving breakthroughs in AI capabilities.
- As a result, we are seeing the rapid release of increasingly larger and more powerful models.

### Moore's Law for Large Language Models

- A variation of Moore's Law has emerged in the context of large language models, suggesting that the size of these models is increasing **10× every year**.

![Moore's law for large language models](image.png)


### Stochastic parrots

> A metaphor used to explain the idea that large language models, although capable of generating believable language, do not actually understand the meaning behind the language they process.

- Earlier language models were based on n-gram methods and could generate contextual content by analyzing multiple words together. This allowed them to consider contexts of over 2,000 words.
- As newer language models have been trained on more and more data, their ability to respond to a wide variety of user queries has improved. However, they still have not become more intelligent — they do not truly understand us better.
