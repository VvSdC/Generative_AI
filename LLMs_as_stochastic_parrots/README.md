# Large Language Models as Stochastic Parrots

---

## ğŸ“Œ Table of Contents

- [Introduction](#introduction)
- [Moore's Law for Large Language Models](#moores-law-for-large-language-models)
- [Stochastic Parrots](#stochastic-parrots)
- [Understanding Stochastic Parrots](#understanding-stochastic-parrots)
- [Behavioral Traits of Stochastic Parrots](#behavioral-traits-of-large-language-models-that-are-stochastic-parrots)

---

## Introduction

- The race to build bigger, faster, and more capable AI models is pushing organizations to stretch the limits of current technology.
- Significant resources and investments are being directed toward achieving breakthroughs in AI capabilities.
- As a result, we are seeing the rapid release of increasingly larger and more powerful models.

<br>

## Moore's Law for Large Language Models

- A variation of Moore's Law has emerged in the context of large language models, suggesting that the size of these models is increasing **10Ã— every year**.

![Moore's law for large language models](image.png)

<br>

## Stochastic Parrots

> A metaphor used to explain the idea that large language models, although capable of generating believable language, do not actually understand the meaning behind the language they process.

- Earlier language models were based on n-gram methods and could generate contextual content by analyzing multiple words together. This allowed them to consider contexts of over 2,000 words.
- As newer language models have been trained on more and more data, their ability to respond to a wide variety of user queries has improved. However, they still have not become more intelligent â€” they do not truly understand us better.

<br>

## Understanding Stochastic Parrots

> Large Language Models (LLMs) generate responses based on probabilities, not understanding.

- Suppose a user types: **"Water is made of __"**
- A language model (e.g., GPT-3.5) tries to complete the sentence by calculating the probability of each possible next word. For instance:
    - hydrogen => 45% probability  
    - Hydrogen => 30% probability  
    - H2O => 15% probability  
    - water => 5% probability  
    - oxygen => 5% probability  

- The word **"hydrogen"** has the highest probability, so it's selected.  
  The sentence becomes: **"Water is made of hydrogen __"**

- Next token predictions:
    - and => 80% probability  
    - , => 10% probability  
    - . => 8% probability  
    - oxygen => 2% probability  

- The model picks **"and"**, leading to: **"Water is made of hydrogen and __"**

- Next token predictions:
    - oxygen => 85% probability  
    - Oxygen => 10% probability  
    - nitrogen => 5% probability  

- The final sentence: **"Water is made of hydrogen and oxygen."**

---

> ğŸ” **Key Insight:** The model doesnâ€™t "know" that water is made of hydrogen and oxygen â€” it simply strings together the most probable words based on its training.

- Despite their impressive generation capabilities, **LLMs are driven purely by probabilistic predictions**, not actual comprehension.
- **Every input (prompt)** results in a completion based on **statistical likelihood of tokens**, not on reasoning or meaning.
- The model outputs what it has *seen before*, with some randomness, but:
  - âŒ It doesnâ€™t understand the **question**
  - âŒ It doesnâ€™t verify the **correctness** of the response
  - âŒ It doesnâ€™t grasp **context** or **appropriateness**
<br>

> âš ï¸ **This lack of true understanding can lead to misleading or harmful outputs, especially in critical applications.**

- This behavior of Large Language Models can be **potentially harmful in the long run**, because the outputs they generate are **probability-driven**, not truth-driven. The responses are often **highly coherent**, making them seem **compelling and believable**, even when incorrect.
- The responses from LLMs are simply **related concepts stitched together probabilistically** â€” not the result of understanding or reasoning.
- Humans are prone to **accepting such fluent text as true**, especially when it sounds logical or natural.
- Language models **do not understand** the **user's context** or the **user's thought process** during communication.
<br>

> ğŸ”„ **Important Note:** Conversation with a Large Language Model is essentially **one-way** â€” the model generates responses without truly engaging, interpreting, or understanding the user.

<br>

## Behavioral Traits of Large Language Models That Are Stochastic Parrots

> These traits highlight the limitations of LLMs due to their probabilistic nature.

- ğŸ” **Repetition of phrases**  
  LLMs often repeat certain words or phrases, especially in longer outputs, due to high-probability token loops.
- ğŸ§± **Templatized / Typical sentence structures**  
  Responses frequently follow common templates or patterns seen during training, lacking originality or variation.
- âŒ **Lack of specific context**  
  The models may miss nuances or fail to respond accurately when the context is subtle, ambiguous, or depends on deeper understanding.
